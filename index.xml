<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Objective Funk on Objective Funk</title>
    <link>http://nsaphra.github.io/</link>
    <description>Recent content in Objective Funk on Objective Funk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>What Does a Coder Do If They Can&#39;t Type?</title>
      <link>http://nsaphra.github.io/post/hands/</link>
      <pubDate>Thu, 08 Aug 2019 18:11:42 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/hands/</guid>
      <description>

&lt;p&gt;In August of 2015, my hands stopped working. I could still control them, but every movement accumulated more pain, so every motion came with a cost: getting dressed in the  morning, sending a text, lifting a glass. In relating this story, I often mention that I spent a month with nothing to do but go to a bar and order a shot of vodka with a straw in it. I then laugh, as though this is a joke.&lt;/p&gt;

&lt;p&gt;I have been in pain for four years.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;talon&#34;&gt;Talon&lt;/h2&gt;

&lt;p&gt;Due to this disability, I cannot type or write. Many people have asked me about the stack that enables me to be productive in spite of this limitation.&lt;/p&gt;

&lt;p&gt;The star of the show is &lt;a href=&#34;https://talonvoice.com/&#34; target=&#34;_blank&#34;&gt;Talon&lt;/a&gt;, a system which makes it easy to write customized grammars and scripts that work with speech recognition systems to enable programmers. Commands range from simple aliases for common symbols to complex meta-commands to repeat a previous utterance or  change dictation modes. For example, just in the case of parentheses, I have separate commands for &lt;code&gt;(&lt;/code&gt;, &lt;code&gt;)&lt;/code&gt;, &lt;code&gt;()&lt;/code&gt;, and &lt;code&gt;()⬅️&lt;/code&gt; (which leaves the cursor between parentheses so my next utterance is bracketed).&lt;/p&gt;

&lt;p&gt;We each have a number of scripts particular to the programmer. My most precious script that I&amp;rsquo;ve written is probably my indexed clipboard:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt; &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; talon.voice &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Key, press, Str, Context
 &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; talon &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; clip
 &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; .talon_community.utils &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

 ctx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Context(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clipboard&amp;#39;&lt;/span&gt;)

 &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;copy_selection&lt;/span&gt;(m):
     &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; clip&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;capture() &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; sel:
         press(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cmd-c&amp;#39;&lt;/span&gt;)
     &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(m&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_words) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
         key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(parse_words(m))
         value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get()
         keymap[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;paste &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; key] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; value
         ctx&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keymap(keymap)
         ctx&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reload()
     &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
         clip&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(sel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get())

 keymap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;paste&amp;#39;&lt;/span&gt;: Key(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cmd-v&amp;#39;&lt;/span&gt;),
     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clip [&amp;lt;dgndictation&amp;gt;]&amp;#39;&lt;/span&gt;: copy_selection,
 }

 ctx&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keymap(keymap)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The use is simple. After selecting a particular phrase using my cursor control commands, I say &amp;ldquo;clip [foo]&amp;ldquo;, and every time I want to enter the same phrase after, I say &amp;ldquo;paste [foo]&amp;ldquo;. I therefore only have to dictate a particularly obnoxious variable name once. However, it does introduce a new challenge: every variable has two names, its written name and its spoken name. The unfortunate side effect is to exacerbate the difficulty of naming variables, which has been called &amp;ldquo;the hardest problem in computer science&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;If you are a vim or Emacs power user, this may all feel familiar to you. I have commands for searching, moving a cursor, selection, and manipulating the clipboard. Learning to dictate code is a lot like learning a new text editor very thoroughly, down to the challenge of customizing for your particular languages and needs.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/dwiel/talon_community&#34; target=&#34;_blank&#34;&gt;Talon community&lt;/a&gt; has specialized commands that take effect depending on application or programming language. For a Perl user, for example, a good starting point might be to borrow settings from Emily Shea:&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/Mz3JeYfBTcY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;p&gt;My Talon setup relies on Dragon for the speech recognition side. Unfortunately, Nuance has discontinued OSX Dragon editions that make scripting possible. The coder behind Talon, &lt;a href=&#34;http://ryanhileman.com/&#34; target=&#34;_blank&#34;&gt;Ryan Hileman&lt;/a&gt;, is working on a suitable replacement but as of the publication of this blog post it is not yet ready.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;interlude&#34;&gt;Interlude&lt;/h3&gt;

&lt;p&gt;People often ask my diagnosis, but it officially depends on the country I&amp;rsquo;m in. After an initial assumption that carpal tunnel was to blame, I was given my first diagnosis: &lt;em&gt;fibromyalgia&lt;/em&gt;, a word which is Doctor for &amp;ldquo;go away&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Later on a neurologist performed a skin biopsy that led to my official American diagnosis of &amp;ldquo;idiopathic small fiber neuropathy&amp;rdquo;, meaning of that I am missing crucial nerve fibers that transmit heat and pain but nobody knows why. &lt;em&gt;Idiopathic&lt;/em&gt; is also Doctor for &amp;ldquo;go away&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I brought my medical records from America to the UK. My British neurologist did not read my records or perform examinations. He gave me my British diagnosis by submitting a note that he had no evidence of any physical cause, and he &amp;ldquo;suspected significant functional overlay&amp;rdquo;, which is how they teach you to call someone delusional in medical school.&lt;/p&gt;

&lt;p&gt;My GP read the note and informed me: He would not prescribe me painkillers. He would not send me for a second opinion from a neurologist, or treatment from any other specialist. The only referral he would write would be to a psychologist to help me &amp;ldquo;resolve the underlying issues behind my pain&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;He then kicked me out of his office for using the word &amp;ldquo;fucking&amp;rdquo;. &amp;ldquo;We do not tolerate cursing&amp;rdquo;, said a sign in the lobby.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;equipment&#34;&gt;Equipment&lt;/h2&gt;

&lt;p&gt;For dictating, I use two different microphones. In the office, I use a &lt;a href=&#34;https://www.amazon.com/Sennheiser-ME-II-Dynamic-Microphone/dp/B00UO0UNOE&#34; target=&#34;_blank&#34;&gt;Sennheiser ME-3&lt;/a&gt;, while for travel I use a Bluetooth headset, the &lt;a href=&#34;https://en-uk.sennheiser.com/mb-pro-1-uc-ml-and-mb-pro-2-uc-ml&#34; target=&#34;_blank&#34;&gt;Sennheiser MB Pro 2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another essential piece of equipment for me is my foot pedal, a &lt;a href=&#34;https://www.pageflip.com/products/firefly&#34; target=&#34;_blank&#34;&gt;PageFlip Firefly&lt;/a&gt;. It is programmable, so I have modified the settings to include one that is useful for reading papers in &lt;a href=&#34;https://skim-app.sourceforge.io/&#34; target=&#34;_blank&#34;&gt;Skim&lt;/a&gt;, with the left pedal corresponding to a click and the right pedal corresponding to down arrow. I can use my feet to scroll, and to click for annotations. Another pedal setting I have added maps the pedals to click and shift+enter. This setting is useful for Jupyter notebooks and writing my research notes and mathematical scratch work in &lt;a href=&#34;https://happenapps.com/&#34; target=&#34;_blank&#34;&gt;Quiver&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When my hands are unusually aggravated, I cannot nudge my mouse around anymore and I fall back on &lt;a href=&#34;https://shortcatapp.com/&#34; target=&#34;_blank&#34;&gt;shortcat&lt;/a&gt;, which allows me to press buttons by dictating keyboard strokes instead of using a mouse.&lt;/p&gt;

&lt;p&gt;My final essential piece of equipment is a pair of &lt;a href=&#34;https://www.futuro-usa.com/3M/en_US/futuro-us/products/~/FUTURO-Night-Wrist-Support/?N=4318+3294508029+3294529207&amp;amp;rt=rud&#34; target=&#34;_blank&#34;&gt;large wrist braces&lt;/a&gt;. The primary purpose of my braces is to discourage me from habitual hand use. I always wear them at conferences, because wearing them is easier than constantly repeating, &amp;ldquo;I cannot shake hands due to a disability&amp;rdquo;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;interlude-1&#34;&gt;Interlude&lt;/h3&gt;

&lt;p&gt;I often struggle to sleep. I dream that my thumbs fall off. I dream that every bone in my hands breaks. I dream that my arms break out in open bleeding sores. I wake up and the pain remains like an invisible nightmare.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;The largest concern if you begin to dictate code is that you do not develop a repetitive stress injury in your vocal tract. I strongly recommend finding a vocal coach who teaches actors and singers how to protect their voices. Speaking quietly can actually cause more damage, hydration is important, and better posture will prevent damage in your voice as well as the rest of your body. It is important to take breaks, and you may find talking tiring outside of work.&lt;/p&gt;

&lt;p&gt;Speech recognition technology is not perfect, and the error rate is even higher if you have an unusual accent. Furthermore, it may force you to take time off from programming every time you develop a cold or sore throat. I live in fear of even minor colds.&lt;/p&gt;

&lt;p&gt;Having a private space to dictate in is essential. I was unable to be productive working from home, but as soon as I had a private office I developed momentum on several research projects. For a lot of people this is a huge limitation, because of the productivity-destroying, soul-sucking trend towards open offices for all programming work. In many countries, large companies will be obligated to provide a space to work in if you are disabled.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;addendum&#34;&gt;Addendum&lt;/h3&gt;

&lt;p&gt;Life with my disability is not easy, but thanks to &lt;a href=&#34;https://en.wikipedia.org/wiki/Hedonic_treadmill&#34; target=&#34;_blank&#34;&gt;hedonic adaptation&lt;/a&gt; as well as satisfying work and &lt;a href=&#34;https://auldreekierollerderby.com/&#34; target=&#34;_blank&#34;&gt;hobbies&lt;/a&gt;, I am actually very happy. If you have recently developed a disability or chronic pain condition, it may feel like you could never adjust to the lifestyle required. I promise that you can, given time and effort. I encourage other disabled scientists and programmers to reach out to me with any questions they have.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://nsaphra.github.io/img/IMG_0167.jpg&#34; alt=&#34;Picture of me smiling while wearing my microphone and wrist braces&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blackbox NLP Panel Discussion</title>
      <link>http://nsaphra.github.io/talk/florence/</link>
      <pubDate>Thu, 25 Jul 2019 13:00:00 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/talk/florence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Dynamics of Language Models</title>
      <link>http://nsaphra.github.io/talk/cuny/</link>
      <pubDate>Tue, 28 May 2019 13:00:00 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/talk/cuny/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Dynamics of Text Models With SVCCA</title>
      <link>http://nsaphra.github.io/talk/amsterdam/</link>
      <pubDate>Tue, 28 May 2019 13:00:00 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/talk/amsterdam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Paying the Panopticon</title>
      <link>http://nsaphra.github.io/talk/panopticon/</link>
      <pubDate>Tue, 26 Feb 2019 13:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/talk/panopticon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparsity Emerges Naturally in Neural Language Models</title>
      <link>http://nsaphra.github.io/publication/saphra-sparsity-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/saphra-sparsity-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Learning Dynamics Of Language Models with SVCCA</title>
      <link>http://nsaphra.github.io/publication/saphra-understanding-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/saphra-understanding-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Model Scheduling</title>
      <link>http://nsaphra.github.io/post/model-scheduling/</link>
      <pubDate>Mon, 13 Aug 2018 15:49:02 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/model-scheduling/</guid>
      <description>

&lt;!--
---
bibliography: &#39;models.bib&#39;
csl: &#39;acm-sigchi.csl&#39;
--- --&gt;

&lt;p&gt;Models can be built incrementally by modifying their hyperparameters
during training. This is most common in transfer learning settings, in
which we seek to adapt the knowledge in an existing model for a new
domain or task. The more general problem of continuous learning is also
an obvious application. Even with a predefined data set, however,
incrementally constraining the topology of the network can offer
benefits as regularization.&lt;/p&gt;

&lt;h2 id=&#34;dynamic-hyperparameters&#34;&gt;Dynamic Hyperparameters&lt;/h2&gt;

&lt;p&gt;The easiest incrementally modified models to train may be those in which
hyperparameters are updated at each epoch. In this case, we do not mean
those hyperparameters associated with network topology, such as the
number or dimension of layers. There are many opportunities to adjust
the topology during training, but the model often requires heavy
retraining in order to impose reasonable structure again, as demonstrated clearly in the case of memory networks&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:25&#34;&gt;&lt;a href=&#34;#fn:25&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. If we
instead focus on the weights associated with regularizers and gates, we
can gradually learn structure without frequent retraining to accommodate
radically altered topologies.&lt;/p&gt;

&lt;h3 id=&#34;curriculum-dropout&#34;&gt;Curriculum Dropout&lt;/h3&gt;

&lt;p&gt;Hinton et al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; describes dropout as reducing overfitting by preventing
co-adaptation of feature detectors which happened to perfectly fit the
data. In this interpretation, co-adaptive clusters of neurons are
concurrently activated. Randomly suppressing these neurons forces them
to develop independence.&lt;/p&gt;

&lt;p&gt;In standard dropout, these co-adaptive neurons are treated as equally
problematic at all stages of training. However, Morerio et. al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:18&#34;&gt;&lt;a href=&#34;#fn:18&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; posit that early
in training, co-adaptation may represent the beginnings of an optimal
self organization of the network. In this view, these structures mainly
pose the threat of overfitting later in training. The authors therefore
introduce a hyperparameter schedule for the dropout ratio, increasing
the rate of dropout as training continues. To the best of my knowledge,
this is the only proposal of adaptive regularization published.&lt;/p&gt;

&lt;h3 id=&#34;mollifying-networks&#34;&gt;Mollifying Networks&lt;/h3&gt;

&lt;p&gt;Mollifying networks&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; are, to my knowledge, the only existing
attempt to combine techniques focused on incrementally manipulating the
distribution of data with techniques focused on incrementally
manipulating the representational capacity of the model. Mollifying
networks incrementally lower the temperature of the data through
simulated annealing while simultaneously modifying various
hyperparameters to permit longer-range dependencies. In the case of an
LSTM, they set the output gate to 1, input gate to \(\frac{1}{t}\), and
forget gate to \(1 - \frac{1}{t}\), where \(t\) is the annealing time step.
Using this system, the LSTM initially behaves as a bag-of-words model,
gradually adding the capacity to handle more context at each time step.&lt;/p&gt;

&lt;p&gt;Mollifying networks use a different data schedule for each layer,
annealing the noise in lower layers faster than in higher layers because
lower-level representations are assumed to learn faster.&lt;/p&gt;

&lt;h2 id=&#34;adaptive-architectures&#34;&gt;Adaptive Architectures&lt;/h2&gt;

&lt;p&gt;The hyperparameters most difficult to modify during training may be
those which dictate the topology of the model architecture itself.
Nonetheless, the deep learning literature contains a long history of
techniques which adapt the model architecture during training, often in
response to the parameters being learned. Methods like these can help
search optimally by smoothing functions at the beginning of training,
speed up learning by starting with a simpler model, or compress a model
to fit easily on a phone or embedded device. Most of these methods could
be classified as either growing a model by adding parameters
mid-training or shrinking a model by pruning edges or nodes.&lt;/p&gt;

&lt;h3 id=&#34;architecture-growth&#34;&gt;Architecture Growth&lt;/h3&gt;

&lt;p&gt;Some recent transfer learning strategies have relied on growing
architectures by creating entire new modules focused on the new task
with connections to the existing network&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:21&#34;&gt;&lt;a href=&#34;#fn:21&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. If the goal is to
instead augment an existing network by adding a small number of
parameters, the problem bears a resemblance to traditional nonparametric
learning, because we need not explicitly limit the model space to begin
with.&lt;/p&gt;

&lt;p&gt;Classical techniques in neural networks such as Cascade Correlation
Networks&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; and Dynamic Node Creation&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; added new nodes at random
one by one and trained them individually. On modern large-scale
architectures and problems, this is intractable. Furthermore, the main
advantage of such methods is that they approach a minimal model, which
is an aim that modern deep learning practitioners no longer consider
valuable thanks to leaps in computing power in the decades since. Modern
techniques for incrementally growing networks must make 2 decisions: 1)
When (and where) do we add new parameters? 2) How do we train new
parameters?&lt;/p&gt;

&lt;p&gt;Warde-Farley et. al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:23&#34;&gt;&lt;a href=&#34;#fn:23&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; add parameters in bulk after training an entire network. The
augmentation takes the form of specialized auxiliary layers added to the
existing network in parallel. These layers are trained on class
boundaries that the original generalist model struggles with. The class
boundaries that require special attention are selected by performing
spectral clustering on the confusion matrix of a holdout data set,
partitioning the classes into challenging subproblems.&lt;/p&gt;

&lt;p&gt;The auxiliary layers are initialized randomly in parallel with the
original generalist system, and then are each trained only on examples
from their assigned partition of the classes. The original generalist
network is held fixed, other than fine-tuning the final classification
layer. The resulting network is a mixture of experts, which was shown to
improve results on an image classification problem.&lt;/p&gt;

&lt;p&gt;Neurogenesis Deep Learning (NDL)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;, meanwhile, makes autoencoders
capable of lifelong learning. This strategy updates the topology of an
autoencoder by adding neurons when the model encounters outliers that it
performs especially poorly on. These new parameters are trained
exclusively on those outliers, allowing the existing decoder parameters
to update with much smaller step sizes. Existing encoder parameters
update only if they are connected directly to the new neuron.&lt;/p&gt;

&lt;p&gt;After introducing and training these new neurons, NDL stabilizes the
existing structure of the network using a method the authors call
&amp;ldquo;intrinsic replay&amp;rdquo;. They reconstruct approximations of previously seen
samples and train on these reconstructions.&lt;/p&gt;

&lt;p&gt;Another system that permits lifelong learning is the infinite Restricted
Boltzmann Machine (RBM) &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;. This extension of the classic RBM
parameterizes hidden units by unique indices, expressing an ordering.
These indices are used to enforce an order on the growth of the network
by favoring older nodes until they have converged, permitting the system
to grow arbitrarily large. An intriguing approach, but it is not obvious
how to apply similar modifications to networks other than the
idiosyncratic generative architecture of the RBM.&lt;/p&gt;

&lt;p&gt;None of these augmentation techniques support recurrent architectures.
In modern natural language processing settings, this is a fatal
limitation. However, it is possible that some of these techniques may be
adapted for RNNs, especially since training specialized subsystems has
been recently tackled in these environments &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&#34;architecture-pruning&#34;&gt;Architecture Pruning&lt;/h3&gt;

&lt;p&gt;Much recent research has focused on the possibility of pruning edges or
entire neurons from trained networks. This approach is promising not
only for the purpose of compression, but potentially as a way of
increasing the generalizability of a network.&lt;/p&gt;

&lt;h4 id=&#34;pruning-edges&#34;&gt;Pruning Edges&lt;/h4&gt;

&lt;p&gt;Procedures that prune edges rather than entire neurons may not reduce
the dimensional type of the network. However, they will make the network
sparser, leading to possible memory savings. A sparser network also
occupies a smaller parameter space, and may therefore still more
general.&lt;/p&gt;

&lt;p&gt;Han et. al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34;&gt;13&lt;/a&gt;&lt;/sup&gt; takes the basic approach of setting weights to 0 if they fall
below a certain threshold. This approach is highly effective for
compression, because the number of weights to be pruned can be easily
modified through the threshold.&lt;/p&gt;

&lt;p&gt;LeCun et. al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34;&gt;14&lt;/a&gt;&lt;/sup&gt; and Hassibi et. al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34;&gt;15&lt;/a&gt;&lt;/sup&gt; both select weights to prune based on Taylor series
approximation of the change in error resulting from trimming. While
these methods were successful for older shallow networks, performing
these operations on an entire network requires a Hessian matrix to be
computed over all parameters, which is generally intractable for deep
modern architectures. Dong et. al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34;&gt;16&lt;/a&gt;&lt;/sup&gt; presents a more efficient alternative by
performing optimal brain surgery over individual layers instead.&lt;/p&gt;

&lt;h4 id=&#34;pruning-nodes&#34;&gt;Pruning Nodes&lt;/h4&gt;

&lt;p&gt;Pruning entire nodes has the advantage of reducing the entire
dimensionality of the network. It also may be faster than choosing
individual edges to prune, because having more nodes than constituent
edges reduces the number of candidates to consider for pruning.&lt;/p&gt;

&lt;p&gt;He et al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34;&gt;17&lt;/a&gt;&lt;/sup&gt; selects which neuron \(w_i^\ell\) to prune from layer \(\ell\)
with width \(d_{\ell}\) by calculating the importance of each node. They
test several importance metrics, finding that the highest performance
results from using the &amp;lsquo;onorm&amp;rsquo;, or average \(l_1\) norm of the activation
pattern of the node:&lt;/p&gt;

&lt;p&gt;\(\mathrm{onorm}(w_i^\ell) = \frac{1}{d_{\ell+1}} \sum_{j = 1}^{d_{\ell+1}} |w_{ij}^{\ell+1}|\)&lt;/p&gt;

&lt;p&gt;Net-trim &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;18&lt;/a&gt;&lt;/sup&gt; likewise relies on the \(l_1\) norm to induce sparsity.&lt;/p&gt;

&lt;p&gt;Wolfe et al.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:24&#34;&gt;&lt;a href=&#34;#fn:24&#34;&gt;19&lt;/a&gt;&lt;/sup&gt; compares the results of importance based pruning to a brute force
method that will greedily select a node to be sacrificed based on its
impact on performance. In the brute force method, they rerun the network
on the test data without each node and sort the nodes according to the
error of the resulting network. Their importance metrics are based on
neuron-level versions of the Taylor series approximations of that impact&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In the first algorithm tested, they rank all nodes according to their
importance and then remove each node in succession. In the second
algorithm, they re-rank the nodes after each removal, in order to
account for the effects of subnetworks that generate and then cancel. In
the second case, they find that it is possible to prune up to 60% of
nodes in a network trained on mnist without significant loss in
performance. This supports an early observation&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:19&#34;&gt;&lt;a href=&#34;#fn:19&#34;&gt;20&lt;/a&gt;&lt;/sup&gt; that the majority
of parameters in a network are unnecessary, and their effect is limited
to generating and then canceling their own noise. The strength of this
effect supports the idea that backpropagation implicitly trains a
minimal network for the task given.&lt;/p&gt;

&lt;p&gt;Srinivas and Babu&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:22&#34;&gt;&lt;a href=&#34;#fn:22&#34;&gt;21&lt;/a&gt;&lt;/sup&gt; prune with the goal of reducing the redundancy of the network, so
they select nodes to remove based on the similarity of their weights to
other neurons in the same layer. Diversity networks&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;, meanwhile,
choose based on the diversity of their activation patterns. In order to
sample a diverse selection of nodes, they use a Determinantal Point
Process. This technique minimizes the dependency between nodes sampled.
They followed this pruning process by &lt;a href=&#34;#merging-nodes&#34;&gt;fusing&lt;/a&gt; the nodes pruned back into
the network.&lt;/p&gt;

&lt;p&gt;An intriguing difference emerges between the observations in these
papers. While Mariet and Sra&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34;&gt;22&lt;/a&gt;&lt;/sup&gt; find that in deeper layers they sample more nodes
from the DPP, Philipp and Carbonell &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:24&#34;&gt;&lt;a href=&#34;#fn:24&#34;&gt;19&lt;/a&gt;&lt;/sup&gt; prune more nodes by brute force in the deeper
layer of a 2-layer network. In other words, diversity networks retain
more nodes at deeper layers while greedy brute force approaches remove
more from the same layers. These results point to fundamental
differences between the respective outcomes of these algorithms and
warrant further investigation.&lt;/p&gt;

&lt;h5 id=&#34;merging-nodes&#34;&gt;Merging Nodes&lt;/h5&gt;

&lt;p&gt;Mariet and Sra&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34;&gt;22&lt;/a&gt;&lt;/sup&gt; found that performance increased after their DPP-based pruning if
they then merged the pruned nodes back into the network. They achieved
this by re-weighting the remaining nodes in the pruned layer to minimize
the difference in activation outputs before and after pruning:&lt;/p&gt;

&lt;p&gt;\( \min_{\tilde{w}_{ij} \in \mathbb{R}} | \sum_{i=1}^k \tilde{w}_{ij} v_i -  \sum_{i=1}^{d_{\ell}} w_{ij} v_i |_2 \)&lt;/p&gt;

&lt;p&gt;Because the DPP is focused on selecting an independent set of neurons,
it seems likely that pruning will select at least 1 node within any
given noise cancellation system to keep, since those cancellation
subnetworks are by necessity highly dependent. The merging step in that
case would merge the noise canceling components back into the noise
generating nodes or vice versa. This would make merging a particular
necessity in diversity networks, but it may still present a tractable
alternative to retraining after a different pruning algorithm.&lt;/p&gt;

&lt;h3 id=&#34;nonparametric-neural-networks&#34;&gt;Nonparametric Neural Networks&lt;/h3&gt;

&lt;p&gt;The pruning and growing strategies are combined in only one work, to my
knowledge. Nonparametric Neural Networks (NNNs)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:20&#34;&gt;&lt;a href=&#34;#fn:20&#34;&gt;23&lt;/a&gt;&lt;/sup&gt; combine adding
neurons with imposing a sparsity-inducing penalty over neurons. For a
feedforward network with \(N^L\) layers, authors introduce 2 such
regularizers, a &amp;ldquo;fan-in&amp;rdquo; and a &amp;ldquo;fan-out&amp;rdquo; variant:&lt;/p&gt;

&lt;p&gt;\(
\Omega_{\mathrm{in}} = \sum_{\ell = 1}^{N^L} \sum_{j = 1}^{d_\ell} \left( \sum_{i = 1}^{d_{\ell}} |w_{ij}^{\ell+1}|^p \right)^{\frac{1}{p}}\)&lt;/p&gt;

&lt;p&gt;\(\Omega_{\mathrm{out}} = \sum_{\ell = 1}^{N^L}  \sum_{i = 1}^{d_{\ell}}  \left( \sum_{j = 1}^{d_\ell+1} |w_{ij}^{\ell}|^p \right)^{\frac{1}{p}}\)&lt;/p&gt;

&lt;p&gt;In other words, the fan-in variant penalizes the \(p\)-norm of the inputs
to each neuron, while the fan-out of variant penalizes the \(p\)-norm of
the outputs from each neuron. In the case of feedforward networks,
either of these regularizers can be added to the loss function with any
positive weight \(\lambda\) and \(0 &amp;lt; p &amp;lt; \infty\) to guarantee that the
objective will converge at some finite number of neurons.&lt;/p&gt;

&lt;p&gt;NNNs offer a combination of beneficial strategies for adapting the
network. In particular with \(p = 1\) or 2, induces sparsity by applying
pressure to form &lt;em&gt;zero-valued neurons&lt;/em&gt;, or neurons which have either a
fan-in or fan-out value of 0. At intervals we can remove these
zero-valued neurons which result. At the same time, we can introduce new
zero-valued neurons at different locations in the network, and the
regularizer guarantees the objective will converge, so we can stop
adding neurons at any point that performance begins to decline.&lt;/p&gt;

&lt;p&gt;However, there are clear issues with this approach. The first obvious
limitation is that this regularizer cannot be applied in any network
with recurrences. This constraint reduces the strategy&amp;rsquo;s usefulness in
many natural language domains where state-of-the-art performance
requires a RNN.&lt;/p&gt;

&lt;p&gt;Another disadvantage to this method is the choice to insert zero-valued
neurons by initializing either the input or output weight vector as 0
and randomly initializing the other associated vector. We therefore
retrain the entire network with each interval, rather than intelligently
initializing and training the new node to accelerate convergence. While
this approach may converge to an optimal number of nodes, it does
nothing to accelerate training or help new nodes specialize.&lt;/p&gt;

&lt;p&gt;Finally, this approach adds and removes entire neurons to create a final
dense network. It therefore forfeits the potential regularization
advantages of the sparser networks which result from instead pruning
weights.&lt;/p&gt;

&lt;h2 id=&#34;teacher-student-approaches&#34;&gt;Teacher/Student Approaches&lt;/h2&gt;

&lt;p&gt;It is also possible to produce a larger or smaller model based on an
existing network by fresh training. When investigating any adaptive
architecture, it is crucial to compare with a baseline which uses the
previous state of the network as a teacher to a student network which
has the new architecture.&lt;/p&gt;

&lt;p&gt;The approach of teacher/student learning, in which the teacher network&amp;rsquo;s
outputs layer are used in lieu of or in addition to true labels, was
introduced in the particular case of distillation learning &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34;&gt;24&lt;/a&gt;&lt;/sup&gt;.
Distillation is a technique for compressing a large ensemble or
generally expensive classifier with high performance. A smaller network
is trained using an objective that combines a loss function applied to true labels with cross-entropy against the logit layer of the large teacher network. In addition to compression, teacher/student learning is effective for domain adaptation technique &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34;&gt;25&lt;/a&gt;&lt;/sup&gt;, suggesting it may be useful for adapting to a new time step in a data schedule.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:25&#34;&gt;Sachan, Mrinmaya, and Eric Xing. &amp;ldquo;Easy questions first? a case study on curriculum learning for question answering.&amp;rdquo; &lt;em&gt;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt; (Volume 1: Long Papers). Vol. 1. 2016.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:25&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. &lt;em&gt;CoRR&lt;/em&gt; abs/1207.0580.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:13&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:18&#34;&gt;Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Rene Vidal, and Vittorio Murino. 2017. Curriculum Dropout. &lt;em&gt;arXiv:1703.06229 [^cs, stat]:&lt;/em&gt;. Retrieved March 22, 2017 from &lt;a href=&#34;http://arxiv.org/abs/1703.06229&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1703.06229&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:18&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. 2016. Mollifying Networks. &lt;em&gt;arXiv:1608.04980 [^cs]:&lt;/em&gt;. Retrieved October 7, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1608.04980&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1608.04980&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In &lt;em&gt;arXiv:1611.01587 [^cs]:&lt;/em&gt;. Retrieved November 11, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1611.01587&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1611.01587&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:10&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:21&#34;&gt;Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive Neural Networks. &lt;em&gt;arXiv:1606.04671 [^cs]:&lt;/em&gt;. Retrieved September 14, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1606.04671&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1606.04671&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:21&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;Scott E. Fahlman and Christian Lebiere. 1989. The cascade-correlation learning architecture. Retrieved November 30, 2016 from &lt;a href=&#34;http://repository.cmu.edu/compsci/1938/&#34; target=&#34;_blank&#34;&gt;http://repository.cmu.edu/compsci/1938/&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Ash. 1989. Dynamic node creation in backpropagation networks. In &lt;em&gt;International 1989 Joint Conference on Neural Networks&lt;/em&gt;, 623 vol.2. &lt;a href=&#34;https://doi.org/10.1109/IJCNN.1989.118509&#34; target=&#34;_blank&#34;&gt;https://doi.org/10.1109/IJCNN.1989.118509&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:23&#34;&gt;David Warde-Farley, Andrew Rabinovich, and Dragomir Anguelov. 2014. Self-informed neural network structure learning. &lt;em&gt;arXiv preprint arXiv:1412.6563&lt;/em&gt;. Retrieved June 9, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1412.6563&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1412.6563&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:23&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Timothy J. Draelos, Nadine E. Miner, Christopher C. Lamb, Craig M. Vineyard, Kristofor D. Carlson, Conrad D. James, and James B. Aimone. 2016. Neurogenesis Deep Learning. &lt;em&gt;arXiv:1612.03770 [^cs, stat]:&lt;/em&gt;. Retrieved February 27, 2017 from &lt;a href=&#34;http://arxiv.org/abs/1612.03770&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1612.03770&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;Marc-Alexandre Cote and Hugo Larochelle. 2015. An Infinite Restricted Boltzmann Machine. &lt;em&gt;arXiv:1502.02476 [^cs]:&lt;/em&gt;. Retrieved February 8, 2018 from &lt;a href=&#34;http://arxiv.org/abs/1502.02476&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1502.02476&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Shlomo E. Chazan, Jacob Goldberger, and Sharon Gannot. 2017. Deep recurrent mixture of experts for speech enhancement. &lt;em&gt;2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)&lt;/em&gt;: 359&amp;ndash;363.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both Weights and Connections for Efficient Neural Networks. &lt;em&gt;arXiv:1506.02626 [^cs]:&lt;/em&gt;. Retrieved May 26, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1506.02626&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1506.02626&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In &lt;em&gt;Advances in neural information processing systems&lt;/em&gt;, 598&amp;ndash;605.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:15&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;Babak Hassibi, David G. Stork, and Gregory J. Wolff. 1993. Optimal brain surgeon and general network pruning. In &lt;em&gt;Neural Networks, 1993., IEEE International Conference on&lt;/em&gt;, 293&amp;ndash;299.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:11&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon. &lt;em&gt;arXiv:1705.07565 [^cs]:&lt;/em&gt;. Retrieved from &lt;a href=&#34;http://arxiv.org/abs/1705.07565&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1705.07565&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu. 2014. Reshaping deep neural network for fast decoding by node-pruning. &lt;em&gt;2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/em&gt;: 245&amp;ndash;249.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:12&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:1&#34;&gt;Aghasi, Alireza, et al. &amp;ldquo;Net-trim: Convex pruning of deep neural networks with performance guarantee.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;. 2017. &lt;a href=&#34;http://arxiv.org/abs/1611.05162&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1611.05162&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:24&#34;&gt;Nikolas Wolfe, Aditya Sharma, Lukas Drude, and Bhiksha Raj. 2017. &amp;ldquo;The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning.&amp;rdquo;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:24&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:19&#34;&gt;Michael C. Mozer and Paul Smolensky. 1989. Using Relevance to Reduce Network Size Automatically.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:19&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:22&#34;&gt;Suraj Srinivas and R. Venkatesh Babu. 2015. Data-free parameter pruning for deep neural networks. &lt;em&gt;arXiv preprint arXiv:1507.06149&lt;/em&gt;. Retrieved October 5, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1507.06149&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1507.06149&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:22&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:17&#34;&gt;Zelda Mariet and Suvrit Sra. 2015. Diversity Networks: Neural Network Compression Using Determinantal Point Processes. &lt;em&gt;arXiv:1511.05077 [^cs]:&lt;/em&gt;. Retrieved February 9, 2018 from &lt;a href=&#34;http://arxiv.org/abs/1511.05077&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1511.05077&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:17&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:20&#34;&gt;George Philipp and Jaime G. Carbonell. 2017. Nonparametric Neural Networks. In &lt;em&gt;arXiv:1712.05440 [^cs]:&lt;/em&gt;. Retrieved February 18, 2018 from &lt;a href=&#34;http://arxiv.org/abs/1712.05440&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1712.05440&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:20&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. &lt;em&gt;arXiv:1503.02531 [^cs, stat]:&lt;/em&gt;. Retrieved September 22, 2016 from &lt;a href=&#34;http://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1503.02531&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:14&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:16&#34;&gt;Jinyu Li, Michael L. Seltzer, Xi Wang, Rui Zhao, and Yifan Gong. 2017. Large-Scale Domain Adaptation via Teacher-Student Learning. &lt;em&gt;arXiv:1708.05466 [^cs]:&lt;/em&gt;. Retrieved August 26, 2017 from &lt;a href=&#34;http://arxiv.org/abs/1708.05466&#34; target=&#34;_blank&#34;&gt;http://arxiv.org/abs/1708.05466&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:16&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>http://nsaphra.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating Informal-Domain Word Representations With UrbanDictionary</title>
      <link>http://nsaphra.github.io/publication/saphra-evaluating-2016/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/saphra-evaluating-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spectral Graph Theory</title>
      <link>http://nsaphra.github.io/post/spectral-graph-theory/</link>
      <pubDate>Fri, 24 Apr 2015 18:11:42 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/spectral-graph-theory/</guid>
      <description>

&lt;p&gt;My goal here is to explain some basic elements of spectral graph theory to myself.
This post may also be useful to an audience with a similar background to mine, including rudimentary linear algebra and lay-computer-scientist exposure to graph theory.&lt;/p&gt;

&lt;p&gt;Spectral graph theory is concerned with the analysis of a graph in terms
of the eigenvectors, algebraic and geometric multiplicities, and other
properties of matrix representations of the graph.
These properties provide a way of efficiently extracting
properties of the graph itself, such as communities and stability.
I may address specific applications in machine learning and representation learning in later posts.&lt;/p&gt;

&lt;p&gt;In this post, the goal is to define and understand the basic properties of the Laplacian matrix, which is just one way to represent a graph in matrix form. The Laplacian is useful for identifying communities and clusters within the graph, and in analyzing the stability of a system.&lt;/p&gt;

&lt;h2 id=&#34;simple-matrix-representations&#34;&gt;Simple Matrix Representations&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with some intuitive representations of a graph $G = (V,E)$.
We will use ${\bf X}(i,j)$ interchangeably with ${\bf X}_{i,j}$ for entries in
matrix ${\bf X}$, and ${\bf x}(i)$ interchangeably with ${\bf x}_i$ for entries in
vector ${\bf{x}}$.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;adjacency matrix&lt;/em&gt; ${\bf A} \in { 0,1 }^{|V| \times |V|}$ tells us which nodes are connected.
We set ${\bf A}_{uv} = 1$ if there is an edge from $u$ to $v$.
If the edge $(u,v) \not\in E$, then the corresponding entry in $A$ is set to 0.&lt;/p&gt;

&lt;p&gt;In an unweighted graph, the adjacency matrix is sufficient for describing $G$.
However, in our analysis we will often rely on information from another matrix characterizing the graph.
The &lt;em&gt;degree matrix&lt;/em&gt; ${\bf D} \in \mathbb{N}^{|V| \times |V|}$ gives us information about how connected a node is.
It is a diagonal matrix in which ${\bf D}_{uu} = {\textrm{deg}(u)}$, which corresponds to the number of edges coming from node $u$.&lt;/p&gt;

&lt;h2 id=&#34;the-walk-matrix&#34;&gt;The Walk Matrix&lt;/h2&gt;

&lt;p&gt;If a matrix represents a graph, what do vectors represent?
A weight vector ${\bf x} \in \mathbb{R}^{|V|}$ assigns some mass to node $v$, which we can retrieve by multiplying the weight vector by a one-hot vector ${\bf v}$, with one bit active at the element corresponding to $v$.
So we can view a weight vector as a function mapping nodes to reals.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;walk matrix&lt;/em&gt; of $G$, defined as ${\bf W} = {\bf A} {\bf D}^{-1}$, provides a way of simulating a random walk through the graph.
If we input a one-hot vector ${\bf v}$, representing a starting point at node $v$, we compute the probability that we end up at node $u$ after one step in a random walk:&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
\left[{\bf W} {\bf v}\right]&lt;em&gt;u &amp;amp;=  \sum&lt;/em&gt;{w \in V} \left[ {\bf A} {\bf D}^{-1} \right]_{uw} {\bf v}&lt;em&gt;w  &lt;br /&gt;
&amp;amp;= \sum&lt;/em&gt;{w \in V} {\bf A}_{uw} \frac{1}{\textrm{deg}(w)} {\bf v}_w&lt;br /&gt;
&amp;amp;= \begin{cases}
    \frac{1}{\textrm{deg}(v)} &amp;amp;\mbox{if } (v,u) \in E &lt;br /&gt;
    0 &amp;amp; \mbox{otherwise}.
\end{cases}
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;Another way to think of this is in terms of the flow of mass.
We start with a mass of 1 on node $v$, and at each timestep diffuses the mass moves down nearby edges.
If we want to model the mass after $k$ timesteps, starting at node $v$, we can compute it with ${\bf W}^k {\bf v}$.&lt;/p&gt;

&lt;!-- TODO: Eigenvalues  --&gt;

&lt;h2 id=&#34;the-laplacian&#34;&gt;The Laplacian&lt;/h2&gt;

&lt;p&gt;To understand the Laplacian matrix, let&amp;rsquo;s first define the Laplacian ${\bf L}$
for a simple graph&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; $G = (V,E)$.
For two node indices $u,v \in V$, the corresponding cell in the Laplacian is given in terms of the degree of each node.
$${\bf L}_{uv} = \begin{cases}
    {\textrm{deg}_u} &amp;amp;\mbox{if } u = v &lt;br /&gt;
    -1 &amp;amp; \mbox{if } (u,v) \in E&lt;br /&gt;
    0 &amp;amp; \mbox{otherwise}.
\end{cases}$$&lt;/p&gt;

&lt;p&gt;For the normalized Laplacian $\mathcal{L}$, we instead say
$$\mathcal{L}_{uv} = \begin{cases}
    1 &amp;amp;\mbox{if } \textrm{$u = v$ and ${\textrm{deg}(v)} \not= 0$} &lt;br /&gt;
    -\frac{1}{\sqrt{ {\textrm{deg}(u)} {\textrm{deg}(v)} }} &amp;amp; \mbox{if } (u,v) \in E&lt;br /&gt;
    0 &amp;amp; \mbox{otherwise}.
\end{cases}$$&lt;/p&gt;

&lt;p&gt;The normalized Laplacian is the form we will use for most analysis.
It is equivalent to normalizing every row and column in the Laplacian.
Let&amp;rsquo;s look at some of its properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Every row and column sums to 0.&lt;/li&gt;
&lt;li&gt;For a graph with no isolated nodes, the diagonals of $\mathcal{L}$ are all 1. In this case, we can decompose the matrix in terms ${\bf D}$ and ${\bf A}$.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Note that
\( {\mathcal{L}}_{uv} = { {\bf L}_{uv} } \frac{1}{\sqrt{ {\textrm{deg}(u)} {\textrm{deg}(v)} }} = {\bf L}_{uv} {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}_u {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}_v \). We therefore know that ${\mathcal{L}}$ decomposes as:&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
{\mathcal{L}}_{uv} &amp;amp;= {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;} {\bf L} {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}
\end{aligned}
$$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The cells
in ${\bf L}$ can be expressed as $${\bf L}&lt;em&gt;{uv} = \begin{cases}
-{\bf A}&lt;/em&gt;{uv} &amp;amp;\mbox{if } u \not= v&lt;br /&gt;
{\bf D}&lt;em&gt;{uu} &amp;amp;\mbox{if } u = v
\end{cases}$$, and in a simple graph we know ${\bf A}&lt;/em&gt;{uu} = 0$ since there are no self-cycles. Therefore, we have the decomposition:&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
{\mathcal{L}} &amp;amp;= {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;} ({\bf D} - {\bf A}) {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}&lt;br /&gt;
&amp;amp;= {\bf I} - {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;} {\bf A} {\bf D}^{-&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}
\end{aligned}
$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;what-does-it-mean&#34;&gt;What does it mean?&lt;/h4&gt;

&lt;p&gt;A intuition for the normalized Laplacian can be gleaned from its product
with a vector ${\bf g} \in \mathbb{R}^{|V|}$.&lt;/p&gt;

&lt;p&gt;$$\begin{aligned}
\left[ {\mathcal{L}} {\bf g} \right]&lt;em&gt;u &amp;amp;= \frac{1}{\sqrt{ {\textrm{deg}(u)} }} \sum&lt;/em&gt;{v: (u,v) \in E} \left(\frac{ {\bf g}_u }{\sqrt{ {\textrm{deg}(u)} }} - \frac{ {\bf g}_v }{\sqrt{ {\textrm{deg}(v)} }} \right)&lt;br /&gt;
&amp;amp;= \frac{1}{\sqrt{ {\textrm{deg}(u)} }} \left({\textrm{deg}(u)} \frac{ {\bf g}&lt;em&gt;u }{\sqrt{ {\textrm{deg}(u)} }} -  \sum&lt;/em&gt;{v: (u,v) \in E} \frac{ {\bf g}_v }{\sqrt{ {\textrm{deg}(v)} }} \right)&lt;br /&gt;
&amp;amp;= {\bf g}&lt;em&gt;u - \sum&lt;/em&gt;{v: (u,v) \in E} \frac{ {\bf g}_v }{\sqrt{ {\textrm{deg}(u)} {\textrm{deg}(v)} }}
\end{aligned}$$&lt;/p&gt;

&lt;p&gt;For a one-hot vector ${\bf g}$ representing node $v$, $[{\mathcal{L}}{\bf g}](v) = 1$ and for all other $u \in V$,
$[{\mathcal{L}}{\bf g}]_u = -\frac{1}{\sqrt{ {\textrm{deg}(u)} {\textrm{deg}(v)} }}$
if $u$ and $v$ are adjacent, and otherwise $[{\mathcal{L}} {\bf g}]_v = 0$.
Note that the sum of output cells for the nodes neighboring $v$ will
be -1, and since the only other nonzero entry will be a 1 (in cell $v$), the
sum of values in the resulting vector will be 0! In fact, any output
vector will have a sum of 0.&lt;/p&gt;

&lt;p&gt;If we activate more bits in ${\bf g}$ in addition to $v$, what happens to
$[{\mathcal{L}}{\bf g}]_u$? If we activate every node in the graph and each
cell receives a uniform weight, then the product will be 0. When else
does this happen? If we start with any node $v$ and then activate cells
in our vector corresponding to each of its neighbors, and go on
recursively for each neighbor, then every time we add a score of 1 for a
node, we also add the sum of scores of its neighbors (always -1), with a
resulting sum of 0 for every cell in the output vector. In general, a
product of 0 occurs if the nodes activated in the input vector form a
closed subgraph – that is, no node has an edge connecting it to a node
that is not activated.&lt;/p&gt;

&lt;!-- Another way to think of the product ${\mathcal{L}}{\bf g}$ is as a potential
function where the nodes with positive weights in ${\bf g}$ “attract” while
their neighbors “repel”. A steady state can be achieved with a vector
representing a closed subgraph, so these attractions and repulsions are
perfectly balanced, resulting in an output vector of 0s.

Notice, however, that a closed subgraph is not the only vector that
induces a steady state! These closed subgraphs form the null space of
the Laplacian. But any eigenvector of the Laplacian will, by definition,
yield a rescaled version of itself. The output vector’s weights on each
node will remain unchanged.

All this gives us a hint about what the Laplacian reveals.
If everyone in your social circle catches a cold, but nobody interacts with people outside of your social circle, the cold will never spread further. You achieved a stable state in which --&gt;

&lt;p&gt;&lt;em&gt;What are the eigenvalues of a Laplacian? What do the eigenvectors of a Laplacian tell us about the graph itself? What does all this have to do with clustering, or community detection, or representation learning? Answers to these questions and more coming soon in the next thrilling installment of this series of blog posts on Spectral Graph Theory!&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;A simple graph is a directed graph with no loops and no more than one edge connecting any two nodes.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>AMRICA: an AMR Inspector for Cross-language Alignments</title>
      <link>http://nsaphra.github.io/publication/naomi-saphra-amrica-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/naomi-saphra-amrica-2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Domain Adaptation for Word Embeddings With Matrix Factorization</title>
      <link>http://nsaphra.github.io/publication/naomi-saphra-domain-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/naomi-saphra-domain-2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A framework for (under) specifying dependency syntax without overloading annotators</title>
      <link>http://nsaphra.github.io/publication/schneider-framework-2014/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/schneider-framework-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Algerian Arabic-French Code-Switched Corpus</title>
      <link>http://nsaphra.github.io/publication/cotterell-algerian-2014/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://nsaphra.github.io/publication/cotterell-algerian-2014/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
