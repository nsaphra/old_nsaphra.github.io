<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Objective Funk</title>
    <link>http://nsaphra.github.io/post/</link>
    <description>Recent content on Objective Funk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://nsaphra.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What Does a Coder Do If They Can&#39;t Type?</title>
      <link>http://nsaphra.github.io/post/hands/</link>
      <pubDate>Thu, 08 Aug 2019 18:11:42 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/hands/</guid>
      <description>In August of 2015, my hands stopped working. I could still control them, but every movement accumulated more pain, so every motion came with a cost: getting dressed in the morning, sending a text, lifting a glass. In relating this story, I often mention that I spent a month with nothing to do but go to a bar and order a shot of vodka with a straw in it. I then laugh, as though this is a joke.</description>
    </item>
    
    <item>
      <title>Model Scheduling</title>
      <link>http://nsaphra.github.io/post/model-scheduling/</link>
      <pubDate>Mon, 13 Aug 2018 15:49:02 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/model-scheduling/</guid>
      <description>Models can be built incrementally by modifying their hyperparameters during training. This is most common in transfer learning settings, in which we seek to adapt the knowledge in an existing model for a new domain or task. The more general problem of continuous learning is also an obvious application. Even with a predefined data set, however, incrementally constraining the topology of the network can offer benefits as regularization.
Dynamic Hyperparameters The easiest incrementally modified models to train may be those in which hyperparameters are updated at each epoch.</description>
    </item>
    
    <item>
      <title>Spectral Graph Theory</title>
      <link>http://nsaphra.github.io/post/spectral-graph-theory/</link>
      <pubDate>Fri, 24 Apr 2015 18:11:42 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/spectral-graph-theory/</guid>
      <description>My goal here is to explain some basic elements of spectral graph theory to myself. This post may also be useful to an audience with a similar background to mine, including rudimentary linear algebra and lay-computer-scientist exposure to graph theory.
Spectral graph theory is concerned with the analysis of a graph in terms of the eigenvectors, algebraic and geometric multiplicities, and other properties of matrix representations of the graph. These properties provide a way of efficiently extracting properties of the graph itself, such as communities and stability.</description>
    </item>
    
    <item>
      <title>Understanding Latent Dirichlet Allocation</title>
      <link>http://nsaphra.github.io/post/lda/</link>
      <pubDate>Mon, 09 Jul 2012 00:00:00 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/lda/</guid>
      <description>[Note - This is a repost of a post I made on my old blog while I was in undergrad. I&amp;rsquo;m including it in case someone finds it useful, since my old blog is defunct. I haven&amp;rsquo;t significantly edited it, so I&amp;rsquo;m sorry if it doesn&amp;rsquo;t fit into my current style.]
This post is directed to a lay CS audience. I am an undergraduate in CS, so I consider myself part of that audience.</description>
    </item>
    
  </channel>
</rss>