<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Objective Funk</title>
    <link>http://nsaphra.github.io/post/</link>
    <description>Recent content on Objective Funk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="http://nsaphra.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What Does a Coder Do If They Can&#39;t Type?</title>
      <link>http://nsaphra.github.io/post/hands/</link>
      <pubDate>Thu, 08 Aug 2019 18:11:42 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/hands/</guid>
      <description>In August of 2015, my hands stopped working. I could still control them, but every movement accumulated more pain, so every motion came with a cost: getting dressed in the morning, sending a text, lifting a glass. I was interning at Google that summer about to begin a PhD in Scotland, but coding all day would have left me in agony. In relating this story, I often mention that for months before I learned to work without my hands, I had nothing to do but go to a bar and order a shot of vodka with a straw in it.</description>
    </item>
    
    <item>
      <title>Model Scheduling</title>
      <link>http://nsaphra.github.io/post/model-scheduling/</link>
      <pubDate>Mon, 13 Aug 2018 15:49:02 +0100</pubDate>
      
      <guid>http://nsaphra.github.io/post/model-scheduling/</guid>
      <description>Models can be built incrementally by modifying their hyperparameters during training. This is most common in transfer learning settings, in which we seek to adapt the knowledge in an existing model for a new domain or task. The more general problem of continuous learning is also an obvious application. Even with a predefined data set, however, incrementally constraining the topology of the network can offer benefits as regularization.
Dynamic Hyperparameters The easiest incrementally modified models to train may be those in which hyperparameters are updated at each epoch.</description>
    </item>
    
    <item>
      <title>Understanding Latent Dirichlet Allocation</title>
      <link>http://nsaphra.github.io/post/lda/</link>
      <pubDate>Mon, 09 Jul 2012 00:00:00 -0400</pubDate>
      
      <guid>http://nsaphra.github.io/post/lda/</guid>
      <description>[Note - This is a repost of a post I made on my old blog while I was in undergrad. I&amp;rsquo;m including it in case someone finds it useful, since my old blog is defunct. I haven&amp;rsquo;t significantly edited it, so I&amp;rsquo;m sorry if it doesn&amp;rsquo;t fit into my current style.]
This post is directed to a lay CS audience. I am an undergraduate in CS, so I consider myself part of that audience.</description>
    </item>
    
  </channel>
</rss>